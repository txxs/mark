(window.webpackJsonp=window.webpackJsonp||[]).push([[90],{479:function(e,r,a){"use strict";a.r(r);var t=a(13),d=Object(t.a)({},(function(){var e=this,r=e.$createElement,a=e._self._c||r;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h3",{attrs:{id:"_1、raft-算法总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、raft-算法总结"}},[e._v("#")]),e._v(" 1、raft 算法总结")]),e._v(" "),a("p",[e._v("Raft 的选举过程")]),e._v(" "),a("p",[e._v("Raft 协议在集群初始状态下是没有 Leader 的, 集群中所有成员均是 Follower，在选举开始期间所有 Follower 均可参与选举，这时所有 Follower 的角色均转变为 Condidate, Leader 由集群中所有的 Condidate 投票选出，最后获得投票最多的 Condidate 获胜，其角色转变为 Leader 并开始其任期，其余落败的 Condidate 角色转变为 Follower 开始服从 Leader 领导。这里有一种意外的情况会选不出 Leader 就是所有 Condidate 均投票给自己，这样无法决出票数多的一方，Raft 算法为了解决这个问题引入了北洋时期袁世凯获选大总统的谋略，即选不出 Leader 不罢休，直到选出为止，一轮选不出 Leader，便令所有 Condidate 随机 sleap（Raft 论文称为 timeout）一段时间，然后马上开始新一轮的选举，这里的随机 sleep 就起了很关键的因素，第一个从 sleap 状态恢复过来的 Condidate 会向所有 Condidate 发出投票给我的申请，这时还没有苏醒的 Condidate 就只能投票给已经苏醒的 Condidate ，因此可以有效解决 Condiadte 均投票给自己的故障，便可快速的决出 Leader。")]),e._v(" "),a("p",[e._v("选举出 Leader 后 Leader 会定期向所有 Follower 发送 heartbeat 来维护其 Leader 地位，如果 Follower 一段时间后未收到 Leader 的心跳则认为 Leader 已经挂掉，便转变自身角色为 Condidate，同时发起新一轮的选举，产生新的 Leader。")]),e._v(" "),a("p",[e._v("Raft 的数据一致性策略")]),e._v(" "),a("p",[e._v("Raft 协议强依赖 Leader 节点来确保集群数据一致性。即 client 发送过来的数据均先到达 Leader 节点，Leader 接收到数据后，先将数据标记为 uncommitted 状态，随后 Leader 开始向所有 Follower 复制数据并等待响应，在获得集群中大于 N/2 个 Follower 的已成功接收数据完毕的响应后，Leader 将数据的状态标记为 committed，随后向 client 发送数据已接收确认，在向 client 发送出已数据接收后，再向所有 Follower 节点发送通知表明该数据状态为committed。")]),e._v(" "),a("p",[e._v("时间被分为很多连续的随机长度的term(一段时间)，一个term由一个唯一的id标识。每个term一开始就进行leader election：")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("followers将自己维护的current_term_id加1。")])]),e._v(" "),a("li",[a("p",[e._v("然后将自己的状态转成candidate。")])]),e._v(" "),a("li",[a("p",[e._v("发送RequestVoteRPC消息(带上current_term_id) 给 其它所有server")])])]),e._v(" "),a("p",[e._v("这个过程会有三种结果：")]),e._v(" "),a("ol",[a("li",[a("p",[e._v("自己被选成了主。当收到了majority的投票后，状态切成leader，并且定期给其它的所有server发心跳消息(其实是不带log的AppendEntriesRPC)以告诉对方自己是current_term_id所标识的term的leader。每个term最多只有一个leader，term id作为logical clock，在每个RPC消息中都会带上，用于检测过期的消息，比如自己是一个过期的leader(term id更小的leader)。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新current_term_id为rpc_term_id，并且如果当前state为leader或者candidate时，将自己的状态切成follower。如果rpc_term_id比本地的current_term_id更小，则拒绝这个RPC消息。")])]),e._v(" "),a("li",[a("p",[e._v("别人成为了主。如1所述，当candidate在等待投票的过程中，收到了大于或者等于本地的current_term_id的声明对方是leader的AppendEntriesRPC时，则将自己的state切成follower，并且更新本地的current_term_id。")])]),e._v(" "),a("li",[a("p",[e._v("没有选出主。当投票被瓜分，没有任何一个candidate收到了majority的vote时，没有leader被选出。这种情况下，每个candidate等待的投票的过程就超时了，接着candidates都会将本地的current_term_id再加1，发起RequestVoteRPC进行新一轮的leader election。")])])]),e._v(" "),a("h4",{attrs:{id:"这个过程尤其好"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#这个过程尤其好"}},[e._v("#")]),e._v(" 这个过程尤其好")]),e._v(" "),a("p",[e._v("http://www.solinx.co/archives/415")]),e._v(" "),a("p",[e._v("1、角色")]),e._v(" "),a("p",[e._v("Raft把集群中的节点分为三种状态：Leader、Follower、Candidate，理所当然每种状态负责的任务也是不一样的，Raft运行时提供服务的时候只存在Leader与Follower两种状态；")]),e._v(" "),a("p",[e._v("Leader（领导者）：负责日志的同步管理，处理来自客户端的请求，与Follower保持这heartBeat的联系；")]),e._v(" "),a("p",[e._v("Follower（追随者）：刚启动时所有节点为Follower状态，响应Leader的日志同步请求，响应Candidate的请求，把请求到Follower的事务转发给Leader；")]),e._v(" "),a("p",[e._v("Candidate（候选者）：负责选举投票，Raft刚启动时由一个节点从Follower转为Candidate发起选举，选举出Leader后从Candidate转为Leader状态；")]),e._v(" "),a("p",[e._v("2、Term")]),e._v(" "),a("p",[e._v("在Raft中使用了一个可以理解为周期（第几届、任期）的概念，用Term作为一个周期，每个Term都是一个连续递增的编号，每一轮选举都是一个Term周期，在一个Term中只能产生一个Leader；先简单描述下Term的变化流程：Raft开始时所有Follower的Term为1，其中一个Follower逻辑时钟到期后转换为Candidate，Term加1这是Term为2（任期），然后开始选举，这时候有几种情况会使Term发生改变：")]),e._v(" "),a("p",[e._v("1：如果当前Term为2的任期内没有选举出Leader或出现异常，则Term递增，开始新一任期选举")]),e._v(" "),a("p",[e._v("2：当这轮Term为2的周期选举出Leader后，过后Leader宕掉了，然后其他Follower转为Candidate，Term递增，开始新一任期选举")]),e._v(" "),a("p",[e._v("3：当Leader或Candidate发现自己的Term比别的Follower小时Leader或Candidate将转为Follower，Term递增")]),e._v(" "),a("p",[e._v("4：当Follower的Term比别的Term小时Follower也将更新Term保持与其他Follower一致；")]),e._v(" "),a("p",[e._v("可以说每次Term的递增都将发生新一轮的选举，Raft保证一个Term只有一个Leader，在Raft正常运转中所有的节点的Term都是一致的，如果节点不发生故障一个Term（任期）会一直保持下去，当某节点收到的请求中Term比当前Term小时则拒绝该请求；")]),e._v(" "),a("p",[e._v("3、选举（Election）\n　　Raft的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为Follower某个节点定时器触发选举后Term递增，状态由Follower转为Candidate，向其他节点发起RequestVote RPC请求，这时候有三种可能的情况发生：")]),e._v(" "),a("p",[e._v("1：该RequestVote请求接收到n/2+1（过半数）个节点的投票，从Candidate转为Leader，向其他节点发送heartBeat以保持Leader的正常运转")]),e._v(" "),a("p",[e._v("2：在此期间如果收到其他节点发送过来的AppendEntries RPC请求，如该节点的Term大则当前节点转为Follower，否则保持Candidate拒绝该请求")]),e._v(" "),a("p",[e._v("3：Election timeout发生则Term递增，重新发起选举")]),e._v(" "),a("p",[e._v("在一个Term期间每个节点只能投票一次，所以当有多个Candidate存在时就会出现每个Candidate发起的选举都存在接收到的投票数都不过半的问题，这时每个Candidate都将Term递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次存在有多个Candidate同时发起投票的问题。\n　　有这么几种情况会发起选举，1：Raft初次启动，不存在Leader，发起选举；2：Leader宕机或Follower没有接收到Leader的heartBeat，发生election timeout从而发起选举;")]),e._v(" "),a("h4",{attrs:{id:"日志复制都是大同小异-超过一半的确认提交就可以提交"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#日志复制都是大同小异-超过一半的确认提交就可以提交"}},[e._v("#")]),e._v(" 日志复制都是大同小异，超过一半的确认提交就可以提交")]),e._v(" "),a("h3",{attrs:{id:"_2、zab协议"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、zab协议"}},[e._v("#")]),e._v(" 2、zab协议")]),e._v(" "),a("p",[e._v("在 ZAB 协议的事务编号 Zxid 设计中，Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的ZXID，并从中读取 epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。")]),e._v(" "),a("p",[e._v("epoch：可以理解为当前集群所处的年代或者周期，每个 leader 就像皇帝，都有自己的年号，所以每次改朝换代，leader 变更之后，都会在前一个年代的基础上加 1。这样就算旧的 leader 崩溃恢复之后，也没有人听他的了，因为 follower 只听从当前年代的 leader 的命令。这有点像raft的周期")]),e._v(" "),a("p",[e._v("值得注意的是，ZAB 提交事务并不像 2PC 一样需要全部 follower 都 ACK，只需要得到 quorum （超过半数的节点）的 ACK 就可以了。")]),e._v(" "),a("p",[e._v("Zookeeper对ZAB协议的具体实现\n协议的 Java 版本实现跟上面的定义有些不同，选举阶段使用的是 Fast Leader Election（FLE），它包含了 Phase 1 的发现职责。因为 FLE 会选举拥有最新提议历史的节点作为 leader，这样就省去了发现最新提议的步骤。实际的实现将 Phase 1 和 Phase 2 合并为 Recovery Phase（恢复阶段）。所以，ZAB 的实现只有三个阶段")]),e._v(" "),a("p",[e._v("A：选举阶段（Fast Leader Election）")]),e._v(" "),a("p",[e._v("B：恢复阶段（Recovery Phase）")]),e._v(" "),a("p",[e._v("C：广播阶段（Broadcast Phase）")]),e._v(" "),a("p",[e._v("Fast Leader Election\n前面提到 FLE 会选举拥有最新提议历史（lastZixd最大）的节点作为 leader，这样就省去了发现最新提议的步骤。这是基于拥有最新提议的节点也有最新提交记录的前提。")]),e._v(" "),a("p",[e._v("成为 leader 的条件")]),e._v(" "),a("p",[e._v("1、选epoch最大的（类似于term的概念）")]),e._v(" "),a("p",[e._v("2、epoch相等，选 zxid 最大的")]),e._v(" "),a("p",[e._v("3、epoch和zxid都相等，选择server id最大的（就是我们配置zoo.cfg中的myid）")]),e._v(" "),a("p",[e._v("节点在选举开始都默认投票给自己，当接收其他节点的选票时，会根据上面的条件更改自己的选票并重新发送选票给其他节点，当有一个节点的得票超过半数，该节点会设置自己的状态为 leading，其他节点会设置自己的状态为 following。")]),e._v(" "),a("p",[e._v("Recovery Phase （恢复阶段）\n这一阶段 follower 发送它们的 lastZixd 给 leader，leader 根据 lastZixd 决定如何同步数据。这里的实现跟前面 Phase 2 有所不同：Follower 收到 TRUNC 指令会中止 L.lastCommittedZxid 之后的提议，收到 DIFF 指令会接收新的提议。")]),e._v(" "),a("p",[e._v("history.lastCommittedZxid：最近被提交的提议的 zxid\nhistory:oldThreshold：被认为已经太旧的已提交提议的 zxid\n广播——主从同步\n主从同步数据比较简单，当有写操作时，如果是从机接收，会转到主机。做一次转发，保证写都是在主机上进行。主先提议事务，收到过半回复后，再发提交。 主收到写操作时，先本地生成事务为事务生成zxid，然后发给所有follower节点。当follower收到事务时，先把提议事务的日志写到本地磁盘，成功后返回给leader。 leader收到过半反馈后对事务提交。再通知所有的follower提交事务，follower收到后也提交事务，提交后就可以对客户端进行分发了。")]),e._v(" "),a("h3",{attrs:{id:"proxay"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#proxay"}},[e._v("#")]),e._v(" proxay")]),e._v(" "),a("p",[e._v("Paxos第一阶段：准备Perpare/诺言Promises")]),e._v(" "),a("p",[e._v("Paxos的第一阶段是prepare/promise，准备阶段就是将建议值发送到各个目标节点。")]),e._v(" "),a("p",[e._v("当建议被发到目标节点后，流程会会检查建议中的序列号，是否是它们见到过的最高级，如果是最高级，它们会发出一个promise不再接受比这个新序列号更旧的建议了，这个诺言promise作为消息从许下诺言的流程发到提交建议新值的流程服务器，这个诺言消息给提交建议的流程后，提交建议的流程需要自己统计一下有多少其他流程已经发回它们的诺言promise了，如果判断数量上是否达到大多数？如果大多数流程已经同意接受这个建议或者更高级序列号的建议，这个提交建议的流程就能知道它获得了发言权(因为有大多数支持)，这样就开始讲话，算法中的下一步处理将可能进行；如果回复诺言的节点数量没有达到大多数，也就是共识没有达成，这样这个节点提交的建议将退出，客户端要求的写操作失败。")]),e._v(" "),a("p",[e._v("为了决定一个建议是否已经有足够的回复诺言promises, 提交建议者只是统计一下它接受到的 promise 消息数量，然后和整个系统中节点服务器数量比较一下，“足够”意味着大多数(N/2 + 1)个流程服务器在某段时间内都回复了诺言promises。如果超过一半的流程服务器没有返回诺言，这意味着这个建议没有被大多数通过，那么在前面描述的读算法中就不能满足大多数的要求，也就不能达成共识，这个建议就退出。其他包括网络分区错误也可能会阻止大多数达成共识，")]),e._v(" "),a("p",[e._v("第二阶段：Paoxs接纳Acceptance")]),e._v(" "),a("p",[e._v("当完成prepare/promise阶段，进入了 propose/accept阶段。")]),e._v(" "),a("p",[e._v("一旦建议提交者已经从大多数其他流程服务器获得了诺言，它会要求许诺的流程服务器接收它们之前承诺接受的新值数据，这是一个“确认commit”阶段，如果没有冲突建议 失败或分区错误，那么这个新建议将被所有其他节点接受，那么Paxos过程就完成了。")]),e._v(" "),a("h3",{attrs:{id:"三者的对比"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三者的对比"}},[e._v("#")]),e._v(" 三者的对比")]),e._v(" "),a("p",[e._v("https://github.com/txxs/interview/blob/master/distri/%E5%88%86%E5%B8%83%E5%BC%8F-cp-zab%E5%92%8Cpaxoy%E5%92%8Craft%E5%8C%BA%E5%88%AB.md")])])}),[],!1,null,null,null);r.default=d.exports}}]);