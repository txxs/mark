(window.webpackJsonp=window.webpackJsonp||[]).push([[137],{516:function(e,r,v){"use strict";v.r(r);var _=v(13),a=Object(_.a)({},(function(){var e=this,r=e.$createElement,v=e._self._c||r;return v("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[v("h3",{attrs:{id:"zookeeper的leader选举"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zookeeper的leader选举"}},[e._v("#")]),e._v(" Zookeeper的Leader选举")]),e._v(" "),v("p",[e._v("原文地址：https://blog.csdn.net/u010758410/article/details/79799035")]),e._v(" "),v("p",[e._v("Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。")]),e._v(" "),v("p",[e._v("一、 服务器初始化启动。")]),e._v(" "),v("p",[e._v("二、 服务器运行期间无法和Leader保持连接。")]),e._v(" "),v("p",[e._v("下面就两种情况进行分析讲解。")]),e._v(" "),v("p",[e._v("1. 服务器启动时期的Leader选举")]),e._v(" "),v("p",[e._v("若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下")]),e._v(" "),v("p",[e._v("(1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。")]),e._v(" "),v("p",[e._v("(2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。")]),e._v(" "),v("p",[e._v("(3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下")]),e._v(" "),v("p",[e._v("- 优先检查ZXID。ZXID比较大的服务器优先作为Leader。")]),e._v(" "),v("p",[e._v("- 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。")]),e._v(" "),v("p",[e._v("对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。")]),e._v(" "),v("p",[e._v("(4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。")]),e._v(" "),v("p",[e._v("(5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。")]),e._v(" "),v("p",[e._v("2. 服务器运行时期的Leader选举")]),e._v(" "),v("p",[e._v("在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。选举过程如下")]),e._v(" "),v("p",[e._v("(1) 变更状态。Leader挂后，余下的非Observer服务器都会讲自己的服务器状态变更为LOOKING，然后开始进入Leader选举过程。")]),e._v(" "),v("p",[e._v("(2) 每个Server会发出一个投票。在运行期间，每个服务器上的ZXID可能不同，此时假定Server1的ZXID为123，Server3的ZXID为122；在第一轮投票中，Server1和Server3都会投自己，产生投票(1, 123)，(3, 122)，然后各自将投票发送给集群中所有机器。")]),e._v(" "),v("p",[e._v("(3) 接收来自各个服务器的投票。与启动时过程相同。")]),e._v(" "),v("p",[e._v("(4) 处理投票。与启动时过程相同，此时，Server1将会成为Leader。")]),e._v(" "),v("p",[e._v("(5) 统计投票。与启动时过程相同。")]),e._v(" "),v("p",[e._v("(6) 改变服务器的状态。与启动时过程相同。")]),e._v(" "),v("p",[e._v("在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法。")]),e._v(" "),v("h3",{attrs:{id:"zk在kafka中的作用"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zk在kafka中的作用"}},[e._v("#")]),e._v(" ZK在kafka中的作用")]),e._v(" "),v("p",[e._v("原文地址：https://blog.csdn.net/Peter_Changyb/article/details/81562855")]),e._v(" "),v("p",[e._v("Kafka使用zk的分布式协调服务，将生产者，消费者，消息储存（broker，用于存储信息，消息读写等）结合在一起。同时借助zk，kafka能够将生产者，消费者和broker在内的所有组件在无状态的条件下建立起生产者和消费者的订阅关系，实现生产者的负载均衡。")]),e._v(" "),v("ol",[v("li",[e._v("broker在zk中注册")])]),e._v(" "),v("p",[e._v("kafka的每个broker（相当于一个节点，相当于一个机器）在启动时，都会在zk中注册，告诉zk其brokerid，在整个的集群中，broker.id/brokers/ids，当节点失效时，zk就会删除该节点，就很方便的监控整个集群broker的变化，及时调整负载均衡。")]),e._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[e._v("topic在zk中注册")])]),e._v(" "),v("p",[e._v("在kafka中可以定义很多个topic，每个topic又被分为很多个分区。一般情况下，每个分区独立在存在一个broker上，所有的这些topic和broker的对应关系都有zk进行维护")]),e._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[e._v("consumer(消费者)在zk中注册")])]),e._v(" "),v("p",[e._v("3.1  注册新的消费者，当有新的消费者注册到zk中，zk会创建专用的节点来保存相关信息，路径ls /consumers/{group_id}/  [ids,owners,offset]，Ids:记录该消费分组有几个正在消费的消费者，Owmners：记录该消费分组消费的topic信息，Offset：记录topic每个分区中的每个offset")]),e._v(" "),v("p",[e._v("3.2 监听消费者分组中消费者的变化 ,监听/consumers/{group_id}/ids的子节点的变化，一旦发现消费者新增或者减少及时调整消费者的负载均衡。")]),e._v(" "),v("h3",{attrs:{id:"zk分布式锁临时结点被谁删除"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zk分布式锁临时结点被谁删除"}},[e._v("#")]),e._v(" ZK分布式锁临时结点被谁删除")]),e._v(" "),v("p",[e._v("在使用ZK分布式锁时，我们使用的是zk的临时节点，当系统宕机或者异常终止导致节点删除没有被显示调用时，锁将不释放。此时只能依靠zk自身的sessionTimeout来删除节点。如果sessionTimeout后依然没有删除节点，考虑是否是zk集群服务器时间不一致的问题。如果持有锁的线程出错，zk通过心跳检测，发现该线程长时间没有应答，session断开，会自动删除锁（临时节点）")]),e._v(" "),v("h3",{attrs:{id:"zk解决什么问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zk解决什么问题"}},[e._v("#")]),e._v(" zk解决什么问题")]),e._v(" "),v("p",[e._v("解决的是：分布式一致性问题")]),e._v(" "),v("p",[e._v("目标一：高性能（简单的数据模型）")]),e._v(" "),v("ol",[v("li",[e._v("采用树形结构组织数据节点；")]),e._v(" "),v("li",[e._v("全量数据节点，都存储在内存中；")]),e._v(" "),v("li",[e._v("Follower 和 Observer 直接处理非事务请求；\n目标二：高可用（构建集群）")]),e._v(" "),v("li",[e._v("半数以上机器存活，服务就能正常运行")]),e._v(" "),v("li",[e._v("自动进行 Leader 选举\n目标三：顺序一致性（事务操作的顺序）")]),e._v(" "),v("li",[e._v("每个事务请求，都会转发给 Leader 处理")]),e._v(" "),v("li",[e._v("每个事务，会分配全局唯一的递增id（zxid，64位：epoch + 自增 id）\n目标四：最终一致性")]),e._v(" "),v("li",[e._v("通过提议投票方式，保证事务提交的可靠性")]),e._v(" "),v("li",[e._v("提议投票方式，只能保证 Client 收到事务提交成功后，半数以上节点能够看到最新数据")])]),e._v(" "),v("h3",{attrs:{id:"zk的应用场景"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zk的应用场景"}},[e._v("#")]),e._v(" ZK的应用场景")]),e._v(" "),v("p",[e._v("原文地址：https://zhuanlan.zhihu.com/p/41953232")]),e._v(" "),v("ol",[v("li",[v("p",[e._v("数据发布与订阅（配置中心）：发布与订阅模型，即所谓的配置中心，顾名思义就是讲发布者将数据发布到zk节点上，共订阅者动态获取数据，实现配置的集中式管理和动态更新。例如，全局的配置信息，服务服务框架的地址列表就非常适合使用。")])]),e._v(" "),v("li",[v("p",[e._v("负载均衡：即软件负载均衡。最典型的是消息中间件的生产、消费者负载均衡。")])]),e._v(" "),v("li",[v("p",[e._v("命名服务(Naming Service)：常见的是发布者将自己的地址列表写到zookeeper的节点上，然后订阅者可以从固定名称的节点获取地址列表，链接到发布者进行相关通讯。")])]),e._v(" "),v("li",[v("p",[e._v("分布式通知/协调：这个利用的是zookeeper的watcher注册和异步通知机制，能够很好的实现分布式环境中不同系统间的通知与协调，实现对数据变更的实时处理。")])]),e._v(" "),v("li",[v("p",[e._v("集群管理与Master选举：集群管理，比如在线率，节点上线下线通知这些。Master选举可以使用临时顺序节点来实现。")])]),e._v(" "),v("li",[v("p",[e._v("分布式锁：分布式锁，这个主要得益于zookeeper数据的强一致性，利用的是临时节点。锁服务分为两类，一个是独占锁，另一个是控制时序。独占，是指所有的客户端都来获取这把锁，最终只能有一个获取到。用的是临时节点。控制时序，所有来获取锁的客户端，都会被安排得到锁，只不过要有个顺序。实际上是某个节点下的临时顺序子节点来实现的。")])]),e._v(" "),v("li",[v("p",[e._v("分布式队列：一种是FIFO，这个就是使用临时顺序节点实现的，和分布式锁服务控制时序一样。第二种是等待队列的成员聚齐之后的才同意按序执行。实际上，是在队列的节点里首先创建一个/queue/num节点，并且赋值队列的大小。这样我们可以通过监控队列节点子节点的变动来感知队列是否已满或者条件已经满足执行的需要。这种，应用场景是有条件执行的任务，条件齐备了之后任务才能执行。")])])]),e._v(" "),v("h3",{attrs:{id:"zk的节点类型"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#zk的节点类型"}},[e._v("#")]),e._v(" ZK的节点类型")]),e._v(" "),v("p",[e._v("原文地址：https://blog.csdn.net/hao495430759/article/details/79994225")]),e._v(" "),v("ol",[v("li",[v("p",[e._v("持久节点（PERSISTENT）:所谓持久节点，是指在节点创建后，就一直存在，直到有删除操作来主动清除这个节点——不会因为创建该节点的客户端会话失效而消失。")])]),e._v(" "),v("li",[v("p",[e._v("持久顺序节点（PERSISTENT_SEQUENTIAL） :这类节点的基本特性和上面的节点类型是一致的。额外的特性是，在ZK中，每个父节点会为他的第一级子节点维护一份时序，会记录每个子节点创建的先后顺序。基于这个特性，在创建子节点的时候，可以设置这个属性，那么在创建节点过程中，ZK会自动为给定节点名加上一个数字后缀，作为新的节点名。这个数字后缀的范围是整型的最大值。 在创建节点的时候只需要传入节点 “/test_”，这样之后，zookeeper自动会给”test_”后面补充数字。")])]),e._v(" "),v("li",[v("p",[e._v("临时节点（EPHEMERAL） :和持久节点不同的是，临时节点的生命周期和客户端会话绑定。也就是说，如果客户端会话失效，那么这个节点就会自动被清除掉。注意，这里提到的是会话失效，而非连接断开。另外，在临时节点下面不能创建子节点。 这里还要注意一件事，就是当你客户端会话失效后，所产生的节点也不是一下子就消失了，也要过一段时间，大概是10秒以内，可以试一下，本机操作生成节点，在服务器端用命令来查看当前的节点数目，你会发现客户端已经stop，但是产生的节点还在。")])]),e._v(" "),v("li",[v("p",[e._v("临时顺序节点（EPHEMERAL_SEQUENTIAL） :此节点是属于临时节点，不过带有顺序，客户端会话结束节点就消失。下面是一个利用该特性的分布式锁的案例流程。\n \n(1)客户端调用create()方法创建名为“locknode/ guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。")])])]),e._v(" "),v("p",[e._v("(2)客户端调用getChildren(“locknode”)方法来获取所有已经创建的子节点，注意，这里不注册任何Watcher。")]),e._v(" "),v("p",[e._v("(3)客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点序号最小，那么就认为这个客户端获得了锁。\n \n(4)如果在步骤3中发现自己并非所有子节点中最小的，说明自己还没有获取到锁。此时客户端需要找到比自己小的那个节点，然后对其调用exist()方法，同时注册事件监听。")]),e._v(" "),v("p",[e._v("(5)之后当这个被关注的节点被移除了，客户端会收到相应的通知。这个时候客户端需要再次调用getChildren(“locknode”)方法来获取所有已经创建的子节点，确保自己确实是最小的节点了，然后进入步骤3。")])])}),[],!1,null,null,null);r.default=a.exports}}]);