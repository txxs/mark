{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[237],{615:function(a,e,s){\"use strict\";s.r(e);var l=s(13),t=Object(l.a)({},(function(){var a=this,e=a.$createElement,s=a._self._c||e;return s(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":a.$parent.slotKey}},[s(\"ol\",[s(\"li\",[a._v(\"ArrayList和linkedlist\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。\")]),a._v(\" \"),s(\"li\",[a._v(\"对于随机访问get和set，ArrayList绝对优于LinkedList，因为LinkedList要移动指针。\")]),a._v(\" \"),s(\"li\",[a._v(\"对于新增和删除操作add和remove，LinkedList比较占优势，因为ArrayList要移动数据。这一点要看实际情况的。若只对单条数据插入或删除，ArrayList的速度反而优于LinkedList。但若是批量随机的插入删除数据，LinkedList的速度大大优于ArrayList.因为ArrayList每插入一条数据，要移动插入点及之后的所有数据。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"2\"}},[s(\"li\",[a._v(\"Hashtable和Hashmap\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（并发下建议使用ConcurrentHashMap ，原因是并发下HashMap的Rehash 会造成元素之间会形成一个循环链表）；\")]),a._v(\" \"),s(\"li\",[a._v(\"效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它；\")]),a._v(\" \"),s(\"li\",[a._v(\"对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。\")]),a._v(\" \"),s(\"li\",[a._v(\"初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证）。也就是说 HashMap 总是使用2的幂作为哈希表的大小。\")]),a._v(\" \"),s(\"li\",[a._v(\"底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"3\"}},[s(\"li\",[a._v(\"HashMap和TreeMap\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"HashMap通过hashcode对其内容进行快速查找，而TreeMap中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用TreeMap（HashMap中元素的排列顺序是不固定的）。\")]),a._v(\" \"),s(\"li\",[a._v(\"在Map中插入、删除和定位元素，HashMap是最好的选择。但如果你要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。使用HashMap要求添加的键类明确定义了hashCode()和equals()的实现。这个TreeMap没有调优选项，因为该树总处于平衡状态。\")]),a._v(\" \"),s(\"li\",[a._v(\"HashMap：数组方式存储key/value，线程非安全，允许null作为key和value，key不可以重复，value允许重复，不保证元素迭代顺序是按照插入时的顺序，key的hash值是先计算key的hashcode值，然后再进行计算，每次容量扩容会重新计算所以key的hash值，会消耗资源，要求key必须重写equals和hashcode方法\")]),a._v(\" \"),s(\"li\",[a._v(\"默认初始容量16，加载因子0.75，扩容为旧容量乘2，查找元素快，如果key一样则比较value，如果value不一样，则按照链表结构存储value，就是一个key后面有多个value；\")]),a._v(\" \"),s(\"li\",[a._v(\"TreeMap：基于红黑二叉树的NavigableMap的实现，线程非安全，不允许null，key不可以重复，value允许重复，存入TreeMap的元素应当实现Comparable接口或者实现Comparator接口，会按照排序后的顺序迭代元素。主要用于存入元素的时候对元素进行自动排序，迭代输出的时候就按排序顺序输出。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"4\"}},[s(\"li\",[a._v(\"Hashmap的结构，1.7和1.8有哪些区别\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"JDK1.7用的是头插法，而JDK1.8及之后使用的都是尾插法，那么他们为什么要这样做呢？因为JDK1.7是用单链表进行的纵向延伸，当采用头插法时会容易出现逆序且环形链表死循环问题。但是在JDK1.8之后是因为加入了红黑树使用尾插法，能够避免出现逆序且链表死循环的问题。\")]),a._v(\" \"),s(\"li\",[a._v(\"扩容后数据存储位置的计算方式也不一样：1.在JDK1.7的时候是直接用hash值和需要扩容的二进制数进行&（这里就是为什么扩容的时候为啥一定必须是2的多少次幂的原因所在，因为如果只有2的n次幂的情况时最后一位二进制数才一定是1，这样能最大程度减少hash碰撞）（hash值 & length-1）\")]),a._v(\" \"),s(\"li\",[a._v(\"在JDK1.7的时候是先扩容后插入的，这样就会导致无论这一次插入是不是发生hash冲突都需要进行扩容，如果这次插入的并没有发生Hash冲突的话，那么就会造成一次无效扩容，但是在1.8的时候是先插入再扩容的，优点其实是因为为了减少这一次无效的扩容，原因就是如果这次插入没有发生Hash冲突的话，那么其实就不会造成扩容，但是在1.7的时候就会急造成扩容\")]),a._v(\" \"),s(\"li\",[a._v(\"而在JDK1.8的时候直接用了JDK1.7的时候计算的规律，也就是扩容前的原始位置+扩容的大小值=JDK1.8的计算方式，而不再是JDK1.7的那种异或的方法。但是这种方式就相当于只需要判断Hash值的新增参与运算的位是0还是1就直接迅速计算出了扩容后的储存方式。\")]),a._v(\" \"),s(\"li\",[a._v(\"JDK1.7的时候使用的是数组+ 单链表的数据结构。但是在JDK1.8及之后时，使用的是数组+链表+红黑树的数据结构（当链表的深度达到8的时候，也就是默认阈值，就会自动扩容把链表转成红黑树的数据结构来把时间复杂度从O（n）变成O（logN）提高了效率）\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"5\"}},[s(\"li\",[a._v(\"为什么在JDK1.8中进行对HashMap优化的时候，把链表转化为红黑树的阈值是8,而不是7或者不是20呢？\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"如果选择6和8（如果链表小于等于6树还原转为链表，大于等于8转为树），中间有个差值7可以有效防止链表和树频繁转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。\")]),a._v(\" \"),s(\"li\",[a._v(\"还有一点重要的就是由于treenodes的大小大约是常规节点的两倍，因此我们仅在容器包含足够的节点以保证使用时才使用它们，当它们变得太小（由于移除或调整大小）时，它们会被转换回普通的node节点，容器中节点分布在hash桶中的频率遵循泊松分布，桶的长度超过8的概率非常非常小。所以作者应该是根据概率统计而选择了8作为阀值\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"6\"}},[s(\"li\",[a._v(\"HashMap 总是使用2的幂作为哈希表的大小的原因（为了加快哈希计算以及减少哈希冲突。）\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"为什么可以加快计算？我们都知道为了找到 KEY 的位置在哈希表的哪个槽里面，需要计算 hash(KEY) % 数组长度，但是！% 计算比 & 慢很多，所以用 & 代替 %，为了保证 & 的计算结果等于 % 的结果需要把 length 减 1。也就是 hash(KEY) & (length - 1)这个 hash(KEY) 没什么可说的，调用 Object 里面的 native 方法完成计算，一般返回的是一个整数，至于是偶数还是奇数就不一定了。\")]),a._v(\" \"),s(\"li\",[a._v(\"为什么可以减少冲突？假设现在数组的长度 length 可能是偶数也可能是奇数。length 为偶数时，length-1 为奇数，奇数的二进制最后一位是 1，这样便保证了 hash &(length-1) 的最后一位可能为 0，也可能为 1（这取决于 h 的值），即 & 运算后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性。而如果 length 为奇数的话，很明显 length-1 为偶数，它的最后一位是 0，这样 hash & (length-1) 的最后一位肯定为 0，即只能为偶数，这样任何 hash 值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"7\"}},[s(\"li\",[a._v(\"为什么HashMap为什么要树化？\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"本质上这是个安全问题。因为在元素放置过程中，如果一个对象哈希冲突，都被放置到同一个桶里，则会形成一个链表，我们知道链表查询是线性的，会严重影响存取的性能。而在现实世界，构造哈希冲突的数据并不是非常复杂的事情，恶意代码就可以利用这些数据大量与服务器端交互，导致服务器端CPU大量占用，这就构成了哈希碰撞拒绝服务攻击，国内一线互联网公司就发生过类似攻击事件。\")]),a._v(\" \"),s(\"li\",[a._v(\"用哈希碰撞发起拒绝服务攻击(DOS，Denial-Of-Service attack),常见的场景是攻击者可以事先构造大量相同哈希值的数据，然后以JSON数据的形式发送给服务器，服务器端在将其构建成为Java对象过程中，通常以Hashtable或HashMap等形式存储，哈希碰撞将导致哈希表发生严重退化，算法复杂度可能上升一个数据级，进而耗费大量CPU资源。\")]),a._v(\" \"),s(\"li\",[a._v(\"链表转红黑树是链表长度达到阈值，这个阈值是多少？阈值是8，红黑树转链表阈值为6。在hash函数设计合理的情况下，发生hash碰撞8次的几率为百万分之6，概率说话。。因为8够用了，至于为什么转回来是6，因为如果hash碰撞次数在8附近徘徊，会一直发生链表和红黑树的转化，为了预防这种情况的发生。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"8\"}},[s(\"li\",[a._v(\"ConcurrentHashMap版本1.7和1.8的区别\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"JDK1.7的ConcurrentHashMap：首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。是由 Segment 数组结构和 HashEntry 数组结构组成。Segment 实现了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。\")]),a._v(\" \"),s(\"li\",[a._v(\"JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）：ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))）synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又有所提升。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"9\"}},[s(\"li\",[a._v(\"HashMap为什么要扰动函数？\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"在 HashMap 存放元素时候有这样一段代码来处理哈希值，这是 java 8 的散列值扰动函数，用于优化散列效果；\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"10\"}},[s(\"li\",[a._v(\"负载因子的作用，为什么负载因子是0.75\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"负载因子越大，数组要被填满时，元素就会越多，元素越多，冲突的几率就会越大，一个链表存的元素也会越多，查询的时候就会越慢。但是，此时空间的利用率更高了——空间换时间\")]),a._v(\" \"),s(\"li\",[a._v(\"负载因此越小，数组要被填满时，元素就会越少，冲突也会也少，一个链表的元素也会越少，查询的时候也就越快。但是，空间的利用率低了——-时间换空间。\")]),a._v(\" \"),s(\"li\",[a._v(\"小于0.5，空着一半就扩容了，这在心理上很多人都会觉得不合理吧，空间肯定会很浪费；但是如果是1的话，只能说有超级大的概率，会发生碰撞，这不符合我们的初衷。当时因为已经设置了hashtable的长度为16。其实负载因子并不重要，重要的其实是那个阈值。负载因子也是为了计算那个阈值的。上面也提到了0.5~1之间找一个小数，乘16可以是一个整数时候0.75很合适。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"11\"}},[s(\"li\",[a._v(\"HashMap的哈希函数怎么设计的？\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"hash 函数是先拿到通过 key 的 hashcode，是 32 位的 int 值，然后让 hashcode 的高 16 位和低 16 位进行异或操作。一定要尽可能降低hash碰撞，越分散越好；算法一定要尽可能高效，因为这是高频操作, 因此采用位运算；因为 key.hashCode()函数调用的是 key 键值类型自带的哈希函数，返回 int 型散列值。int 值范围为-2147483648~2147483647，前后加起来大概 40 亿的映射空间。只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。\")])]),a._v(\" \"),s(\"ol\",{attrs:{start:\"12\"}},[s(\"li\",[a._v(\"遇到并发的情况如何处理\")])]),a._v(\" \"),s(\"ul\",[s(\"li\",[a._v(\"写时复制是指：在并发访问的情景下，当需要修改JAVA中Containers的元素时，不直接修改该容器，而是先复制一份副本，在副本上进行修改。修改完成之后，将指向原来容器的引用指向新的容器(副本容器)。\")]),a._v(\" \"),s(\"li\",[a._v(\"写时复制带来的影响：①由于不会修改原始容器，只修改副本容器。因此，可以对原始容器进行并发地读。其次，实现了读操作与写操作的分离，读操作发生在原始容器上，写操作发生在副本容器上。②数据一致性问题：读操作的线程可能不会立即读取到新修改的数据，因为修改操作发生在副本上。但最终修改操作会完成并更新容器，因此这是最终一致性。\")]),a._v(\" \"),s(\"li\",[a._v(\"CopyOnWriteArrayList add (需要锁）：如果多个线程同时去写，多线程写的时候会Copy出N个副本出来，那么可能内存花销很大，所以用一个锁ReetrantLock锁住，一次只能一个线程去添加，这个线程是可重入的。 CopyOnWriteArrayList get (需要锁）：get直接get即可，因为获取的旧的值所以没有影响。\")]),a._v(\" \"),s(\"li\",[a._v(\"Java 中有 HashTable、Collections.synchronizedMap、以及ConcurrentHashMap可以实现线程安全的Map。HashTable是直接在操作方法上加synchronized关键字，锁住整个数组，粒度比较大，Collections.synchronizedMap 是使用 Collections 集合工具的内部类，通过传入 Map 封装出一个 SynchronizedMap 对象，内部定义了一个对象锁，方法内通过对象锁实现；ConcurrentHashMap 使用分段锁，降低了锁粒度，让并发度大大提高。\")])])])}),[],!1,null,null,null);e.default=t.exports}}]);","extractedComments":[]}